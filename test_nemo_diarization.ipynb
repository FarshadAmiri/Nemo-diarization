{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa180b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: D:\\Git_repos\\Nemo-diarization\n",
      "Python path includes: D:\\Git_repos\\Nemo-diarization\n",
      "✓ Imports successful (module reloaded)\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory to notebook location\n",
    "notebook_dir = Path(r\"D:\\Git_repos\\Nemo-diarization\")\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# Add to Python path\n",
    "if str(notebook_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes: {notebook_dir}\")\n",
    "\n",
    "# Reload module to get latest changes\n",
    "import importlib\n",
    "if 'nemo_diarization' in sys.modules:\n",
    "    importlib.reload(sys.modules['nemo_diarization'])\n",
    "\n",
    "# Import the main function\n",
    "from nemo_diarization import process_audio_with_nemo, diarize_and_transcribe\n",
    "\n",
    "print(\"✓ Imports successful (module reloaded)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faafad3",
   "metadata": {},
   "source": [
    "## Create Voice Embeddings Database (Optional)\n",
    "\n",
    "If you want to identify specific speakers by name, create a database of known voices first.\n",
    "Otherwise, skip this section and speakers will be labeled as SPEAKER_00, SPEAKER_01, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create voice embeddings database for speaker identification\n",
    "You need reference audio samples for each person you want to identify\n",
    "\"\"\"\n",
    "\n",
    "# Option 1: Create database from audio samples\n",
    "# Prepare your speaker samples - each person should have 1-3 audio clips\n",
    "speaker_samples = {\n",
    "    \"sp3000\": [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\3000\\15664\\3000-15664-0024.flac\", \n",
    "               r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\3000\\15664\\3000-15664-0040.flac\"],\n",
    "    \"sp777\" : [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\777\\126732\\777-126732-0028.flac\",\n",
    "               r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\777\\126732\\777-126732-0025.flac\"],\n",
    "    \"sp422\" : [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\422\\122949\\422-122949-0021.flac\",\n",
    "               r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\422\\122949\\422-122949-0016.flac\"],\n",
    "    \"sp1993\": [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\1993\\147964\\1993-147964-0005.flac\",\n",
    "               r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\1993\\147964\\1993-147964-0003.flac\"],\n",
    "}\n",
    "\n",
    "# Database output path\n",
    "voice_embeddings_database_path = r\"D:\\Git_repos\\Nemo-diarization\\outputs\\db\\speakers_db.json\"\n",
    "\n",
    "# Uncomment to create the database:\n",
    "from resemblyzer import VoiceEncoder\n",
    "import json\n",
    "\n",
    "encoder = VoiceEncoder()\n",
    "embeddings_db = {}\n",
    "\n",
    "for speaker_name, audio_files in speaker_samples.items():\n",
    "    print(f\"Processing {speaker_name}...\")\n",
    "    speaker_embeddings = []\n",
    "    \n",
    "    for audio_file in audio_files:\n",
    "        from resemblyzer import preprocess_wav\n",
    "        wav = preprocess_wav(audio_file)\n",
    "        embedding = encoder.embed_utterance(wav)\n",
    "        speaker_embeddings.append(embedding.tolist())\n",
    "    \n",
    "    # Average embeddings for better accuracy\n",
    "    import numpy as np\n",
    "    avg_embedding = np.mean(speaker_embeddings, axis=0)\n",
    "    embeddings_db[speaker_name] = avg_embedding.tolist()\n",
    "    print(f\"  ✓ {speaker_name} enrolled\")\n",
    "\n",
    "# Save database\n",
    "import os\n",
    "os.makedirs(os.path.dirname(voice_embeddings_database_path), exist_ok=True)\n",
    "with open(voice_embeddings_database_path, 'w') as f:\n",
    "    json.dump(embeddings_db, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Database saved to: {voice_embeddings_database_path}\")\n",
    "print(f\"✓ Enrolled {len(embeddings_db)} speakers\")\n",
    "\n",
    "print(\"To create embeddings database:\")\n",
    "print(\"1. Prepare audio samples for each speaker\")\n",
    "print(\"2. Update the speaker_samples dictionary above\")\n",
    "print(\"3. Uncomment the code block\")\n",
    "print(\"4. Run this cell\")\n",
    "print(\"\\nOr skip this if you only need anonymous speaker labels (SPEAKER_00, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c0db5",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your paths and parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e3dde2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file to process\n",
    "meeting_audio_path = r\"D:\\Projects_tmp\\noisy_audio_files\\speeches\\1\\concat_1.wav\"\n",
    "# meeting_audio_path = r\"D:\\Projects_tmp\\noisy_audio_files\\speeches\\1\\concat_1_1m.wav\"\n",
    "# meeting_audio_path = r\"D:\\Projects_tmp\\noisy_audio_files\\speeches\\1\\concat_1_doubled.wav\"\n",
    "\n",
    "# Voice embeddings database (can be empty for basic diarization)\n",
    "voice_embeddings_database_path = r\"D:\\Git_repos\\Nemo-diarization\\outputs\\db\\speakers_db.json\"\n",
    "\n",
    "# Language (e.g., 'en', 'fa', 'ar', or None for auto-detect)\n",
    "expected_language = \"en\"\n",
    "\n",
    "# Enable transcription\n",
    "output_transcriptions = False\n",
    "\n",
    "# Path to your cached Whisper model (optional)\n",
    "# Examples:\n",
    "# - Small model: r\"path/to/whisper/small.pt\"\n",
    "# - Medium model: r\"path/to/whisper/medium.pt\"  \n",
    "# - Persian finetuned: r\"path/to/whisper/persian_finetuned.pt\"\n",
    "transcriptor_model_path = None  # Will use default \"base\" model if None\n",
    "\n",
    "# Number of expected speakers (optional, auto-detect if None)\n",
    "num_speakers = None\n",
    "\n",
    "# Backend selection:\n",
    "# - use_wsl=True: WSL2 NeMo with GPU (RECOMMENDED - accurate! Now using native WSL venv for speed)\n",
    "# - use_wsl=False: Windows pyannote.audio (fallback option)\n",
    "# \n",
    "# Note: NeMo now uses native WSL venv (~nemo_venv) instead of mounted drive\n",
    "# This fixes the slow import issue while keeping GPU acceleration!\n",
    "use_wsl = True  # Changed back to True - NeMo with GPU is working now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac928e",
   "metadata": {},
   "source": [
    "## Run Diarization\n",
    "\n",
    "Simple one-line execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43ecba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NVIDIA NeMo SPEAKER DIARIZATION PIPELINE\n",
      "======================================================================\n",
      "Audio file: D:\\Projects_tmp\\noisy_audio_files\\speeches\\1\\concat_1.wav\n",
      "Voice database: D:\\Git_repos\\Nemo-diarization\\outputs\\db\\speakers_db.json\n",
      "Language: en\n",
      "Transcription: disabled\n",
      "Mode: WSL2 NeMo\n",
      "======================================================================\n",
      "\n",
      "[WSL2 Mode] Running NeMo diarization in WSL2 Ubuntu...\n",
      "Executing NeMo diarization in WSL...\n",
      "Note: First run may take longer while models download\n",
      "\n",
      "\n",
      "✓ NeMo diarization completed\n",
      "\n",
      "[Speaker Identification] Matching speakers to database...\n",
      "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
      "✓ Merged 12 segments → 8 segments\n",
      "\n",
      "======================================================================\n",
      "RESULTS SUMMARY\n",
      "======================================================================\n",
      "Number of speakers: 4\n",
      "Total segments: 8\n",
      "Output files: {'rttm': '/mnt/d/Projects_tmp/noisy_audio_files/speeches/1/nemo_output/pred_rttms/concat_1.rttm'}\n"
     ]
    }
   ],
   "source": [
    "# Run the diarization pipeline\n",
    "result = process_audio_with_nemo(\n",
    "    meeting_audio_path=meeting_audio_path,\n",
    "    voice_embeddings_database_path=voice_embeddings_database_path,\n",
    "    expected_language=expected_language,\n",
    "    output_transcriptions=output_transcriptions,\n",
    "    transcriptor_model_path=transcriptor_model_path,\n",
    "    num_speakers=num_speakers,\n",
    "    use_wsl=use_wsl\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Number of speakers: {result['num_speakers']}\")\n",
    "print(f\"Total segments: {len(result['segments'])}\")\n",
    "if 'transcription' in result:\n",
    "    print(f\"Detected language: {result.get('detected_language', 'N/A')}\")\n",
    "print(f\"Output files: {result['output_files']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0379d",
   "metadata": {},
   "source": [
    "## View Diarization Results\n",
    "\n",
    "Inspect the diarization segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fedfa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All segments:\n",
      "----------------------------------------------------------------------\n",
      "1. [0.54s - 20.75s] sp3000\n",
      "2. [21.18s - 33.31s] sp422\n",
      "3. [33.58s - 39.71s] sp777\n",
      "4. [39.98s - 44.10s] sp422\n",
      "5. [44.10s - 58.35s] sp3000\n",
      "6. [58.86s - 69.39s] sp777\n",
      "7. [69.74s - 82.35s] sp1993\n",
      "8. [82.86s - 95.47s] sp3000\n"
     ]
    }
   ],
   "source": [
    "# Display first 10 diarization segments\n",
    "print(\"All segments:\")\n",
    "print(\"-\" * 70)\n",
    "for i, seg in enumerate(result['segments'], 1):\n",
    "    print(f\"{i}. [{seg['start']:.2f}s - {seg['end']:.2f}s] {seg['speaker']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de51f9d",
   "metadata": {},
   "source": [
    "## View Transcription with Speakers\n",
    "\n",
    "If transcription was enabled, view the transcribed text with speaker labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34fdc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display transcription with speakers\n",
    "if 'speaker_segments' in result:\n",
    "    print(\"\\nTranscription with Speaker Labels:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for seg in result['speaker_segments'][:20]:  # First 20 segments\n",
    "        print(f\"\\n[{seg['start']:.2f}s - {seg['end']:.2f}s] {seg['speaker']}:\")\n",
    "        print(f\"  {seg['text']}\")\n",
    "else:\n",
    "    print(\"Transcription not available. Set output_transcriptions=True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6e567",
   "metadata": {},
   "source": [
    "## Full Transcription Text\n",
    "\n",
    "View the complete transcribed text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dacee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full transcription\n",
    "if 'transcription' in result:\n",
    "    print(\"Full Transcription:\")\n",
    "    print(\"=\"*70)\n",
    "    print(result['transcription'])\n",
    "else:\n",
    "    print(\"Transcription not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0b9b8",
   "metadata": {},
   "source": [
    "## Alternative: Simplified Function\n",
    "\n",
    "You can also use the simplified wrapper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the simplified function\n",
    "result2 = diarize_and_transcribe(\n",
    "    meeting_audio_path=meeting_audio_path,\n",
    "    expected_language=\"en\",\n",
    "    output_transcriptions=True,\n",
    "    transcriptor_model_path=None  # Use default model\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(result2['segments'])} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528777ec",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save results to different formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save to JSON\n",
    "output_file = \"diarization_output.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_file}\")\n",
    "\n",
    "# Save transcription to text file\n",
    "if 'speaker_segments' in result:\n",
    "    transcript_file = \"transcript_with_speakers.txt\"\n",
    "    with open(transcript_file, 'w', encoding='utf-8') as f:\n",
    "        for seg in result['speaker_segments']:\n",
    "            f.write(f\"[{seg['start']:.2f}s - {seg['end']:.2f}s] {seg['speaker']}:\\n\")\n",
    "            f.write(f\"{seg['text']}\\n\\n\")\n",
    "    \n",
    "    print(f\"✓ Transcript saved to: {transcript_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204c4e3",
   "metadata": {},
   "source": [
    "## Testing with Different Whisper Models\n",
    "\n",
    "Test with your cached Whisper models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test with different models\n",
    "\n",
    "# Test with small model\n",
    "# result_small = diarize_and_transcribe(\n",
    "#     meeting_audio_path=meeting_audio_path,\n",
    "#     expected_language=\"en\",\n",
    "#     transcriptor_model_path=r\"D:\\path\\to\\whisper_small.pt\"\n",
    "# )\n",
    "\n",
    "# Test with medium model\n",
    "# result_medium = diarize_and_transcribe(\n",
    "#     meeting_audio_path=meeting_audio_path,\n",
    "#     expected_language=\"en\",\n",
    "#     transcriptor_model_path=r\"D:\\path\\to\\whisper_medium.pt\"\n",
    "# )\n",
    "\n",
    "# Test with Persian finetuned model\n",
    "# result_persian = diarize_and_transcribe(\n",
    "#     meeting_audio_path=r\"path\\to\\persian_audio.wav\",\n",
    "#     expected_language=\"fa\",\n",
    "#     transcriptor_model_path=r\"D:\\path\\to\\whisper_persian_finetuned.pt\"\n",
    "# )\n",
    "\n",
    "print(\"Uncomment the examples above to test with your cached models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

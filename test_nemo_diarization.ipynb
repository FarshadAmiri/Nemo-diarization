{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa180b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: D:\\Git_repos\\Nemo-diarization\n",
      "Python path includes: D:\\Git_repos\\Nemo-diarization\n",
      "✓ Imports successful (module reloaded)\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory to notebook location\n",
    "notebook_dir = Path(r\"D:\\Git_repos\\Nemo-diarization\")\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# Add to Python path\n",
    "if str(notebook_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes: {notebook_dir}\")\n",
    "\n",
    "# Reload module to get latest changes\n",
    "import importlib\n",
    "if 'nemo_diarization' in sys.modules:\n",
    "    importlib.reload(sys.modules['nemo_diarization'])\n",
    "\n",
    "# Import functions\n",
    "from nemo_diarization import diarize_with_nemo, add_transcription_to_segments, diarize_and_transcribe\n",
    "\n",
    "print(\"✓ Imports successful (module reloaded)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faafad3",
   "metadata": {},
   "source": [
    "## Create Voice Embeddings Database (Optional)\n",
    "\n",
    "If you want to identify specific speakers by name, create a database of known voices first.\n",
    "Otherwise, skip this section and speakers will be labeled as SPEAKER_00, SPEAKER_01, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdf7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Create voice embeddings database for speaker identification\n",
    "# You need reference audio samples for each person you want to identify\n",
    "# \"\"\"\n",
    "\n",
    "# # Option 1: Create database from audio samples\n",
    "# # Prepare your speaker samples - each person should have 1-3 audio clips\n",
    "# speaker_samples = {\n",
    "#     \"sp3000\": [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\3000\\15664\\3000-15664-0024.flac\", \n",
    "#                r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\3000\\15664\\3000-15664-0040.flac\"],\n",
    "#     \"sp777\" : [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\777\\126732\\777-126732-0028.flac\",\n",
    "#                r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\777\\126732\\777-126732-0025.flac\"],\n",
    "#     \"sp422\" : [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\422\\122949\\422-122949-0021.flac\",\n",
    "#                r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\422\\122949\\422-122949-0016.flac\"],\n",
    "#     \"sp1993\": [r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\1993\\147964\\1993-147964-0005.flac\",\n",
    "#                r\"D:\\Projects_tmp\\noisy_audio_files\\LibriSpeech\\dev-clean\\1993\\147964\\1993-147964-0003.flac\"],\n",
    "# }\n",
    "\n",
    "# # Database output path\n",
    "# voice_embeddings_database_path = r\"D:\\Git_repos\\Nemo-diarization\\outputs\\db\\speakers_db.json\"\n",
    "\n",
    "# # Uncomment to create the database:\n",
    "# from resemblyzer import VoiceEncoder\n",
    "# import json\n",
    "\n",
    "# encoder = VoiceEncoder()\n",
    "# embeddings_db = {}\n",
    "\n",
    "# for speaker_name, audio_files in speaker_samples.items():\n",
    "#     print(f\"Processing {speaker_name}...\")\n",
    "#     speaker_embeddings = []\n",
    "    \n",
    "#     for audio_file in audio_files:\n",
    "#         from resemblyzer import preprocess_wav\n",
    "#         wav = preprocess_wav(audio_file)\n",
    "#         embedding = encoder.embed_utterance(wav)\n",
    "#         speaker_embeddings.append(embedding.tolist())\n",
    "    \n",
    "#     # Average embeddings for better accuracy\n",
    "#     import numpy as np\n",
    "#     avg_embedding = np.mean(speaker_embeddings, axis=0)\n",
    "#     embeddings_db[speaker_name] = avg_embedding.tolist()\n",
    "#     print(f\"  ✓ {speaker_name} enrolled\")\n",
    "\n",
    "# # Save database\n",
    "# import os\n",
    "# os.makedirs(os.path.dirname(voice_embeddings_database_path), exist_ok=True)\n",
    "# with open(voice_embeddings_database_path, 'w') as f:\n",
    "#     json.dump(embeddings_db, f, indent=2)\n",
    "\n",
    "# print(f\"\\n✓ Database saved to: {voice_embeddings_database_path}\")\n",
    "# print(f\"✓ Enrolled {len(embeddings_db)} speakers\")\n",
    "\n",
    "# print(\"To create embeddings database:\")\n",
    "# print(\"1. Prepare audio samples for each speaker\")\n",
    "# print(\"2. Update the speaker_samples dictionary above\")\n",
    "# print(\"3. Uncomment the code block\")\n",
    "# print(\"4. Run this cell\")\n",
    "# print(\"\\nOr skip this if you only need anonymous speaker labels (SPEAKER_00, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c0db5",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your paths and parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e3dde2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file to process\n",
    "meeting_audio_path = r\"D:\\Projects_tmp\\noisy_audio_files\\speeches\\1\\concat_1.wav\"\n",
    "\n",
    "# Voice embeddings database (can be empty for basic diarization)\n",
    "voice_embeddings_database_path = r\"D:\\Git_repos\\Nemo-diarization\\outputs\\db\\speakers_db.json\"\n",
    "\n",
    "# Language (e.g., 'en', 'fa', 'ar', or None for auto-detect)\n",
    "expected_language = \"en\"\n",
    "\n",
    "# Whisper model for transcription ('tiny', 'base', 'small', 'medium', 'large')\n",
    "whisper_model = \"medium\"\n",
    "\n",
    "# Number of expected speakers (optional, auto-detect if None)\n",
    "num_speakers = None\n",
    "\n",
    "# Backend: use_wsl=True for NeMo (GPU), False for pyannote (Windows)\n",
    "use_wsl = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac928e",
   "metadata": {},
   "source": [
    "## Run Diarization (Fast!)\n",
    "\n",
    "NeMo diarization with GPU - completes in ~10 seconds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ecba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NVIDIA NeMo SPEAKER DIARIZATION\n",
      "======================================================================\n",
      "Audio file: D:\\Projects_tmp\\noisy_audio_files\\speeches\\1\\concat_1.wav\n",
      "Voice database: D:\\Git_repos\\Nemo-diarization\\outputs\\db\\speakers_db.json\n",
      "Mode: WSL2 NeMo GPU\n",
      "======================================================================\n",
      "\n",
      "[WSL2 Mode] Running NeMo diarization in WSL2 Ubuntu...\n",
      "Executing NeMo diarization in WSL...\n",
      "Note: First run may take longer while models download\n",
      "\n",
      "\n",
      "✓ NeMo diarization completed\n",
      "\n",
      "[Speaker Identification] Matching speakers to database...\n",
      "Loaded the voice encoder model on cpu in 0.03 seconds.\n",
      "✓ Merged 12 segments → 8 segments\n",
      "\n",
      "======================================================================\n",
      "DIARIZATION RESULTS\n",
      "======================================================================\n",
      "Number of speakers: 4\n",
      "Total segments: 8\n",
      "Output files: {'rttm': '/mnt/d/Projects_tmp/noisy_audio_files/speeches/1/nemo_output/pred_rttms/concat_1.rttm'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Diarization (fast - GPU accelerated)\n",
    "result = diarize_with_nemo(\n",
    "    meeting_audio_path=meeting_audio_path,\n",
    "    voice_embeddings_database_path=voice_embeddings_database_path,\n",
    "    num_speakers=num_speakers,\n",
    "    use_wsl=use_wsl\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIARIZATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Number of speakers: {result['num_speakers']}\")\n",
    "print(f\"Total segments: {len(result['segments'])}\")\n",
    "print(f\"Output files: {result['output_files']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0379d",
   "metadata": {},
   "source": [
    "## View Diarization Results\n",
    "\n",
    "Inspect the diarization segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fedfa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All segments:\n",
      "----------------------------------------------------------------------\n",
      "1. [0.54s - 20.75s] sp3000\n",
      "2. [21.18s - 33.31s] sp422\n",
      "3. [33.58s - 39.71s] sp777\n",
      "4. [39.98s - 44.10s] sp422\n",
      "5. [44.10s - 58.35s] sp3000\n",
      "6. [58.86s - 69.39s] sp777\n",
      "7. [69.74s - 82.35s] sp1993\n",
      "8. [82.86s - 95.47s] sp3000\n"
     ]
    }
   ],
   "source": [
    "# Display first 10 diarization segments\n",
    "print(\"All segments:\")\n",
    "print(\"-\" * 70)\n",
    "for i, seg in enumerate(result['segments'], 1):\n",
    "    print(f\"{i}. [{seg['start']:.2f}s - {seg['end']:.2f}s] {seg['speaker']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb6be1",
   "metadata": {},
   "source": [
    "## Add Transcription\n",
    "\n",
    "Add Whisper transcription to the diarization results (runs on Windows):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "510c5de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Transcription] Loading Whisper 'medium' model...\n",
      "[Transcription] Transcribing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\venvs\\venv_nemo\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 9561/9561 [00:50<00:00, 190.14frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transcription] Aligning with speaker segments...\n",
      "✓ Transcription complete\n",
      "✓ Detected language: en\n",
      "✓ Transcribed 22 segments\n",
      "\n",
      "======================================================================\n",
      "TRANSCRIPTION RESULTS\n",
      "======================================================================\n",
      "Detected language: en\n",
      "Transcribed segments: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Add transcription (optional - runs on Windows with Whisper)\n",
    "result = add_transcription_to_segments(\n",
    "    diarization_result=result,\n",
    "    expected_language=expected_language,\n",
    "    model_name=whisper_model\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRANSCRIPTION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Detected language: {result['detected_language']}\")\n",
    "print(f\"Transcribed segments: {len(result['speaker_segments'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de51f9d",
   "metadata": {},
   "source": [
    "## View Transcription with Speakers\n",
    "\n",
    "If transcription was enabled, view the transcribed text with speaker labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c34fdc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription with Speaker Labels:\n",
      "======================================================================\n",
      "\n",
      "[0.00s - 6.74s] sp3000:\n",
      "  Arctic beauty and desolation, with their blessings and dangers, all may be found here, to test\n",
      "\n",
      "[6.74s - 9.82s] sp3000:\n",
      "  the endurance and skill of adventurous climbers.\n",
      "\n",
      "[10.28s - 16.20s] sp3000:\n",
      "  But far better than climbing the mountain is going around its warm, fertile base, enjoying\n",
      "\n",
      "[16.20s - 20.38s] sp3000:\n",
      "  its bounties like a bee circling around a bank of flowers.\n",
      "\n",
      "[21.18s - 26.80s] sp422:\n",
      "  The distinctions of moral values have either originated in a ruling caste pleasantly conscious\n",
      "\n",
      "[26.80s - 32.14s] sp422:\n",
      "  of being different from the ruled, or among the ruled class, the slaves and dependents\n",
      "\n",
      "[32.14s - 33.08s] sp422:\n",
      "  of all sorts.\n",
      "\n",
      "[33.80s - 38.18s] sp777:\n",
      "  Stevie, accustomed to move about disregarded, had got up from the kitchen table carrying\n",
      "\n",
      "[38.18s - 39.68s] sp777:\n",
      "  off his drawing to bed with him.\n",
      "\n",
      "[40.18s - 43.98s] sp422:\n",
      "  We truthful ones, the nobility in ancient Greece called themselves.\n",
      "\n",
      "[44.62s - 50.32s] sp3000:\n",
      "  Perhaps the profession of doing good may be full, but everybody should be kind at least\n",
      "\n",
      "[50.32s - 51.04s] sp3000:\n",
      "  to himself.\n",
      "\n",
      "[51.92s - 57.50s] sp3000:\n",
      "  Thus one saunters on and on in the glorious radiance, in utter peace and forgetfulness\n",
      "\n",
      "[57.50s - 58.22s] sp3000:\n",
      "  of time.\n",
      "\n",
      "[59.02s - 63.20s] sp777:\n",
      "  The sheet of paper covered with circles dropped out of his fingers, and he remained staring\n",
      "\n",
      "[63.20s - 68.22s] sp777:\n",
      "  at the old terrorist, as if rooted suddenly to the spot by his morbid horror and dread\n",
      "\n",
      "[68.22s - 69.16s] sp777:\n",
      "  of physical pain.\n",
      "\n",
      "[69.78s - 74.42s] sp1993:\n",
      "  They sought about the house most of the day, as if it were Sunday, greasing their boots,\n",
      "\n",
      "[74.68s - 77.28s] sp1993:\n",
      "  mending their suspenders, plating whiplashes.\n",
      "\n",
      "[78.16s - 82.12s] sp1993:\n",
      "  Anyway, he would never allow one of his horses to be put to such a strain.\n"
     ]
    }
   ],
   "source": [
    "# Display transcription with speakers\n",
    "if 'speaker_segments' in result:\n",
    "    print(\"\\nTranscription with Speaker Labels:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for seg in result['speaker_segments'][:20]:  # First 20 segments\n",
    "        print(f\"\\n[{seg['start']:.2f}s - {seg['end']:.2f}s] {seg['speaker']}:\")\n",
    "        print(f\"  {seg['text']}\")\n",
    "else:\n",
    "    print(\"Transcription not available. Set output_transcriptions=True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6e567",
   "metadata": {},
   "source": [
    "## Full Transcription Text\n",
    "\n",
    "View the complete transcribed text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec7dacee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Transcription:\n",
      "======================================================================\n",
      " Arctic beauty and desolation, with their blessings and dangers, all may be found here, to test the endurance and skill of adventurous climbers. But far better than climbing the mountain is going around its warm, fertile base, enjoying its bounties like a bee circling around a bank of flowers. The distinctions of moral values have either originated in a ruling caste pleasantly conscious of being different from the ruled, or among the ruled class, the slaves and dependents of all sorts. Stevie, accustomed to move about disregarded, had got up from the kitchen table carrying off his drawing to bed with him. We truthful ones, the nobility in ancient Greece called themselves. Perhaps the profession of doing good may be full, but everybody should be kind at least to himself. Thus one saunters on and on in the glorious radiance, in utter peace and forgetfulness of time. The sheet of paper covered with circles dropped out of his fingers, and he remained staring at the old terrorist, as if rooted suddenly to the spot by his morbid horror and dread of physical pain. They sought about the house most of the day, as if it were Sunday, greasing their boots, mending their suspenders, plating whiplashes. Anyway, he would never allow one of his horses to be put to such a strain. Yet strange to say, there are days, even here, somewhat dull-looking, when the mountain seems uncommunicative, sending out no appreciable invitation, as if not at home.\n"
     ]
    }
   ],
   "source": [
    "# Display full transcription\n",
    "if 'transcription' in result:\n",
    "    print(\"Full Transcription:\")\n",
    "    print(\"=\"*70)\n",
    "    print(result['transcription'])\n",
    "else:\n",
    "    print(\"Transcription not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0b9b8",
   "metadata": {},
   "source": [
    "## All-in-One: Diarization + Transcription\n",
    "\n",
    "Or use the convenience function that does both steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncomment above to run complete pipeline in one call\n"
     ]
    }
   ],
   "source": [
    "# Complete pipeline in one call (diarization + transcription)\n",
    "# Commented out by default - uncomment to use\n",
    "# result_combined = diarize_and_transcribe(\n",
    "#     meeting_audio_path=meeting_audio_path,\n",
    "#     voice_embeddings_database_path=voice_embeddings_database_path,\n",
    "#     expected_language=expected_language,\n",
    "#     transcriptor_model_name=whisper_model,\n",
    "#     num_speakers=num_speakers,\n",
    "#     use_wsl=use_wsl\n",
    "# )\n",
    "\n",
    "# print(f\"✓ Complete! {result_combined['num_speakers']} speakers, {len(result_combined['speaker_segments'])} transcribed segments\")\n",
    "\n",
    "print(\"Uncomment above to run complete pipeline in one call\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528777ec",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save results to different formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33d6db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to: diarization_output.json\n",
      "✓ Transcript saved to: transcript_with_speakers.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save to JSON\n",
    "output_file = \"diarization_output.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_file}\")\n",
    "\n",
    "# Save transcription to text file\n",
    "if 'speaker_segments' in result:\n",
    "    transcript_file = \"transcript_with_speakers.txt\"\n",
    "    with open(transcript_file, 'w', encoding='utf-8') as f:\n",
    "        for seg in result['speaker_segments']:\n",
    "            f.write(f\"[{seg['start']:.2f}s - {seg['end']:.2f}s] {seg['speaker']}:\\n\")\n",
    "            f.write(f\"{seg['text']}\\n\\n\")\n",
    "    \n",
    "    print(f\"✓ Transcript saved to: {transcript_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204c4e3",
   "metadata": {},
   "source": [
    "## Testing with Different Whisper Models\n",
    "\n",
    "Test with your cached Whisper models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test with different models\n",
    "\n",
    "# Test with small model\n",
    "# result_small = diarize_and_transcribe(\n",
    "#     meeting_audio_path=meeting_audio_path,\n",
    "#     expected_language=\"en\",\n",
    "#     transcriptor_model_path=r\"D:\\path\\to\\whisper_small.pt\"\n",
    "# )\n",
    "\n",
    "# Test with medium model\n",
    "# result_medium = diarize_and_transcribe(\n",
    "#     meeting_audio_path=meeting_audio_path,\n",
    "#     expected_language=\"en\",\n",
    "#     transcriptor_model_path=r\"D:\\path\\to\\whisper_medium.pt\"\n",
    "# )\n",
    "\n",
    "# Test with Persian finetuned model\n",
    "# result_persian = diarize_and_transcribe(\n",
    "#     meeting_audio_path=r\"path\\to\\persian_audio.wav\",\n",
    "#     expected_language=\"fa\",\n",
    "#     transcriptor_model_path=r\"D:\\path\\to\\whisper_persian_finetuned.pt\"\n",
    "# )\n",
    "\n",
    "print(\"Uncomment the examples above to test with your cached models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
